# -*- coding: utf-8 -*-
"""Employee.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12rO4e5YAN-Mt6sebSRAjyKB-DbiMoagX

# <font size="+3" ><b> <center><u>Employee Data Analysis</u></center></b></font><br><a id="top"></a>

# <font color='seagreen'>Table of Contents:</font>
1. Installing library
2. Importing libraries and Loading Dataset
      
      2.1 Importing Libraries

      2.2 Extracting dataset

      2.3 Examining the dataset

      2.4 Checking for missing data

      2.5 Simple way to analyze this data using Pandas Profiling

3. Data Cleaning

      3.1 Handling missing values

      3.2 Handling Duplicate Data

4. Exploratory Data Analysis

      4.1 Features Analysis

      4.2 Visualize the distribution of the data for each variable

      4.3 Departement Analysis

      4.4 Region Analysis

      4.5 Education Analysis

      4.6 Age Analysis

      4.7 Gender Analysis

      4.8 Recruitment Analysis

      4.9 Total Training analysis

      4.10 Last Year's Rating Analysis

      4.11 Years of Service Analysis

      4.12 KPI > 80% Analysis

      4.13 Award Analysis

      4.14 Average Training Score Analysis

      4.15 Total and Perncetage Promoted Employee

      4.16 Correlation between Fwatures

5. Feature Engineering

      5.1 One Hot Encoding

      5.2 Label Encoder

      5.3 Data Scaling

6. Feature Selection
7. Model Building

      7.1 Models Selection
      
      - 7.1.1 Choose a model based on the value of accuracy
      - 7.1.2 Choose a model based on the value of accuracy with Cross Validation

      7.2 Hyper-Parameter Tuning using HalvingGridSearchCV
      7.3 AUC and ROC Analysis

# 1. Installing Library
"""

!pip install pandas-profiling==2.7.1
!pip install plotly==4.5.2
!pip install -U scikit-learn
!pip install catboost
!pip install -U imbalanced-learn

"""#**2. Importing libraries and Loading Dataset**

##**2.1 Importing Libraries**

Python may be a phenomenal dialect with a dynamic community that produces numerous astonishing libraries. So, I'm planning to present a few of the specified libraries for presently, and as time goes on, we'll keep unboxing modern libraries as we regard vital.
"""

import numpy as np
import pandas as pd
from pandas_profiling import ProfileReport
import missingno as msno 
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report,f1_score
from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import HalvingGridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier 
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier as knn
from sklearn.naive_bayes import GaussianNB as GB


from imblearn.combine import SMOTETomek

"""##**2.2 Extracting dataset**"""

df = pd.read_excel('/content/dataset_test_ds.xlsx')

"""## **2.3 Examining the dataset**"""

df.head(10)

"""**Inference:**

* We have EmployeeID, Departement, Region, Educational qualification, Gender, Recruitment Type, Total Training, Age, etc.
"""

df.info()

print('Dataframe dimensions:', df.shape)

"""This dataset has 54808 employee detail data in total and 14 different feature."""

df.dtypes

"""We have 8 integer, 1 float and 5 object datatypes in our dataset"""

df.describe()

"""We can see for each the mean, max, min, q1, median, q3, ect values ​​for each variable.

##**2.4 Checking for missing data**
Datasets within real world are regularly untidy. Let's analyze and see what we have here.
"""

msno.matrix(df)

"""As per our inference, we can visualize the null values in **pendidikan and rating_tahun_lalu** columns. Let's see the count"""

print('Data columns with null values:',df.isnull().sum(), sep = '\n')

# gives some infos on columns types and numer of null values
tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})
tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))
tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T.
                         rename(index={0:'null values (%)'}))
display(tab_info)

df[df['rating_tahun_lalu'].isna()]

df[df['rating_tahun_lalu'].isna() & df['masa_kerja']==1]

"""For rating_tahun_lalu can't drop or fill it with mean/median values and it will go against the context of this dataset. Because they don't have less than one year of experience for performance appraisal."""

df[df['pendidikan'].isna()]

"""**Inference**

* There are 2409 null values in pendidikan column.
* There are 4124 null values in rating_tahun_lalu column. Which means they still work less than 1 year.
* For rating_tahun_lalu can't impute it with mean/median values and it will go against the context of this dataset.
* Our best way to deal with these null values is to impute it with '0' which shows they still work less than 1 year.

##**2.5 Simple way to analyze this data using Pandas Profiling**
"""

ProfileReport(df)

"""#**3. Data Cleaning**

##**3.1 Handling missing values**
First lets focus on the missing values in dataset.
"""

print(df['pendidikan'].value_counts())

# Get the mode of the feature pendidikan
print()
print('Mode: ' , df['pendidikan'].mode()[0])

# Replace the missing values for pendidikan with mode
df['pendidikan'].fillna(df['pendidikan'].mode()[0] , inplace = True)

# gives some infos on columns types and numer of null values
tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})
tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))
tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T.
                         rename(index={0:'null values (%)'}))
display(tab_info)

"""OK, therefore, by fill with mode in this column these entries we end up with a dataframe filled at 100% for all variables! Then I will fill missing value in rating_tahun_lalu column."""

df['rating_tahun_lalu'].fillna(value=0, inplace=True)
print('Dataframe dimensions:', df.shape)

# gives some infos on columns types and numer of null values
tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})
tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))
tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T.
                         rename(index={0:'null values (%)'}))
display(tab_info)

msno.bar(df, color='y')

"""Yayy ! we have cleared that rating_tahun_lalu with zero null values. Now it's time to drop unwanted features !

## **3.2 Handling Duplicate Data**
"""

df[df.duplicated()].shape

"""Nothing Duplicated data

#**4. Data Analysis**

##**4.1 Features Analysis**
"""

sns.pairplot(df)

"""* We can see the relationship between variables in this dataset.
* We will drop the Employee ID variable because it has nothing to do with any other variables.
"""

# Create a copy of the train dataset
data = df.copy()
data.drop('id_karyawan' , inplace = True , axis = 1)

# Replace f & m in Gender column
data['jenis_kelamin'].replace('f', 'Female', inplace=True)
data['jenis_kelamin'].replace('m', 'Male', inplace=True)

"""##**4.2 Visualize the distribution of the data for each variable**"""

data.hist(edgecolor='black', 
                        linewidth=1, 
                        figsize=(15, 15))
plt.show()

"""**Inference:**

* Most employees have 1 time of training.

* Most employees are from 30 to 35 years old in the company.

* Most employees have the last year's rating between 3.0 to 3.5.

* Most employees have below 10 years of service.

* Many employees do not meet the KPIs (key performance indicator).

* The employees who won the awards equal less than 1/50 of the number of those who did not win.

* More employees have the average training score of 50 and 60 than other scores.

* Approximately 1/10 of employees are promoted.

##**4.3 Departement Analysis**
"""

fig = px.histogram(data, x=data['departemen'], color=data['departemen'], title= 'Total Employee by departement'.title())
fig.update_layout(showlegend=False)
fig.show()

"""The maximum number of employees in each department is from Sales & Marketing and Operations. To the very least from the R&D and Legal departments."""

fig = px.histogram(data, x=data['departemen'], color='dipromosikan', title= 'Total Promoted Employee by departement'.title())
fig.update_layout(showlegend=False)
fig.show()

"""**Inference:**
* Sales & Marketing 1213 employees were promoted.
* Operations there are 1023 employees who were promoted.
* Technology there are 768 employees who were promoted.
* Analytics there are 512 employees promoted.
* R&D there are 69 employees who are promoted.
* Procurement there are 688 employees who were promoted.
* Finance there are 206 employees who are promoted.
* HR there are 136 employees who are promoted.
* Legally there are 53 employees who were promoted.

So the department with the most promoted employees is Sales & Marketing, and the fewest is from the Legal department.
"""

data_pct =data.groupby(['departemen','dipromosikan'])['dipromosikan'].count().unstack()
data_pct.columns = ['Not Promoted', 'Promoted']
data_pct['Percentage'] = 100 * data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])
data_pct = data_pct.sort_values('Percentage', ascending=False)

fig = go.Figure()
fig.add_trace(go.Scatter(x=data_pct.index, y=data_pct['Percentage']))
fig.update_layout(title = 'Percentage Promoted Employee by Departement', xaxis_tickfont_size = 14, yaxis=dict(title='% of Promoted Employee'))
fig.show()

"""<b>Insights drawn from the above plots:
1. Technology Dept. has the highest promotion rate across all deparments.
2. Legal Dept. has the lowest promotion rate across all departments.
3. Top 3 Departments to work in on the basis of Promotions are : Technology, Procurement, Analysis</b>

##**4.4 Region Analysis**
"""

fig = px.histogram(data, x=data['wilayah'], color=data['wilayah'], title= 'Total Employee by region'.title(), color_discrete_sequence=px.colors.qualitative.G10)
fig.update_layout(showlegend=False)
fig.show()

"""**Conclusion:**

Region_2 has the highest number of employees and Region_18 has the least number of employees.

"""

fig = px.histogram(data, x=data['wilayah'], color='dipromosikan', title= 'Total Promoted Employee by region'.title(), 
                   color_discrete_sequence=px.colors.qualitative.Set2)
fig.update_layout(showlegend=False)
fig.show()

"""**Conclusion:**
* Region_2 has 989 promoted employees.
* Region_22 has 734 promoted employees.
* Region_7 has 516 employees promoted.

"""

data_pct =data.groupby(['wilayah','dipromosikan'])['dipromosikan'].count().unstack()
data_pct.columns = ['Not Promoted', 'Promoted']
data_pct['Percentage'] = 100 * data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])
data_pct = data_pct.sort_values('Percentage', ascending=False)
data_pct = data_pct[0:10]
data_pct = data_pct.sort_values('Percentage', ascending=True)

fig = px.bar(data_pct,y=data_pct.index, x=data_pct['Percentage'], color=data_pct.index, text='Percentage', orientation='h', 
             title="The most promoted employee percentage by region".title(), color_discrete_sequence=px.colors.qualitative.Safe_r)
fig.update_layout(showlegend=False)
fig.show()

"""Insights drawn from the above plot:

Top 3 Locales in terms of Advancement Rates : Region_4, Region_17, Region_25. Employees in these districts are working difficult and are getting compensated for the same.

##**4.5 Education Analysis**
"""

fig = px.histogram(data, x=data['pendidikan'], color=data['pendidikan'], title= 'Total Employee by education'.title(),
                   color_discrete_sequence=px.colors.qualitative.Pastel)
fig.update_layout(showlegend=False)
fig.show()

"""**Conclusion:**

Ranking Employee Education Level: 

  1. Bachelor's, 
  2. Master's or above,  
  3. Below secondary.

Because this company need resouces with skill.
"""

fig = px.histogram(data, x=data['pendidikan'], color='dipromosikan', title= 'Total Promoted Employee by education'.title(),
                  color_discrete_sequence=px.colors.sequential.Bluyl)
fig.update_layout(showlegend=False)
fig.show()

"""<b>Insights drawn from the above plot:
1.   Bachelor's 3130 promoted employees.
2.   Bachelor's 1471 promoted employees.
3.   Below Secondary 67 promoted employees.
"""

data_pct =data.groupby(['pendidikan','dipromosikan'])['dipromosikan'].count().unstack()
    data_pct.columns = ['Not Promoted', 'Promoted']
    data_pct['Percentage'] = data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])
    data_pct = data_pct.sort_values('Percentage', ascending=True)

    colors = px.colors.sequential.Peach

    fig = go.Figure(data=[go.Pie(labels=data_pct.index, values=data_pct['Percentage'], hole=.3, hoverinfo='label+percent', textinfo='label+percent')])
    fig.update_layout(title_text="percentage promoted employee by education".title())
    fig.update_traces(marker=dict(colors=colors, line=dict(color='#000000', width=2)))
    fig.show()

"""**Insights drawn from the above plots:**
1. Maste'r & above has the highest promotion rate across all education level.
2. Bachelor's has the lowest promotion rate across all education level.

##**4.6 Age Analysis**
"""

plt.subplots(figsize=(15,5))
sns.distplot(data['umur'])
plt.title('Distribution of Age of Employees', fontsize = 30)

"""Conclusion:
* This company is dominated by employees aged 30-35 years.
"""

# Compute a simple cross tabulation of two factors
data_pct = pd.crosstab(data['umur'], data['dipromosikan'])

# Plot the stacked bar plot
plot = data_pct.div(data_pct.sum(1).astype(float), axis = 0).plot(kind = 'bar', 
                                                   stacked = True, 
                                                   color = ['seagreen', 'yellow'],
                                                   figsize = (20, 5))

# Add a title and labels
plt.title('Correlation between Age and Promotion of employees')
plt.xlabel('Age')
plt.legend(['Not Promoted', 'Is Promoted'], loc='lower right')

# Adjust the lables on x-axis
plot.set_xticklabels(
    plot.get_xticklabels(),
    rotation=360,
    horizontalalignment='right');

plt.show()

"""Conclusion:
* There is no big difference for employees promoted by age.

Employee age is not an indicator of employees being promoted

##**4.7 Gender Analysis**
"""

fig = px.histogram(data, x=data['jenis_kelamin'], color=data['jenis_kelamin'], title= 'Total Employee by Gender'.title(),
                   color_discrete_sequence=px.colors.qualitative.Pastel)
fig.update_layout(showlegend=False)
fig.show()

data_pct =data.groupby(['jenis_kelamin','dipromosikan'])['dipromosikan'].count().unstack()
    data_pct.columns = ['Not Promoted', 'Promoted']
    data_pct['Percentage'] = data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])
    data_pct = data_pct.sort_values('Percentage', ascending=True)

    colors = px.colors.sequential.Peach

    fig = go.Figure(data=[go.Pie(labels=data_pct.index, values=data_pct['Percentage'], hole=.3, hoverinfo='label+percent', textinfo='label+percent')])
    fig.update_layout(title_text="promoted employee percentage by gender".title())
    fig.update_traces(marker=dict(colors=colors, line=dict(color='#000000', width=2)))
    fig.show()

"""**Inference:**

1. There are 38,496 employees is male.
2. There are 16,312 employees is female.
3. But, percetage of promoted employees have different. Female have a higher percentage than male.
"""

data_pct =data.groupby(['jenis_kelamin','pendidikan','dipromosikan'])['dipromosikan'].count().unstack()
    data_pct.columns = ['Not Promoted', 'Promoted']
    data_pct['Percentage'] = data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])

    colors = px.colors.qualitative.Pastel2

    fig = go.Figure(data=[go.Pie(labels=data_pct.index, values=data_pct['Percentage'], hoverinfo='label+percent', textinfo='percent')])
    fig.update_layout(title_text="promoted employee percentage by gender and education".title())
    fig.update_traces(marker=dict(colors=colors, line=dict(color='#000000', width=2)))
    fig.show()

"""**Insight:**

1. Female with education master&above have higher percentage for promoted.
2. Male with education master&above have second ranking percentage for promoted.
3. Male with education below secondary third ranking percentage for promoted.
4. Female with education bachelor's have next ranking percentage for promoted.
5. Male with education bachelor's have next ranking percentage for promoted.
6. Female with education below secondary have lower percentage for promoted.

##**4.8 Recruitment Analysis**
"""

fig = px.histogram(data, x=data['rekrutmen'], color=data['dipromosikan'], title= 'Total Employee by Recruitment'.title(),
                   color_discrete_sequence=px.colors.qualitative.Pastel)
fig.update_layout(showlegend=False)
fig.show()

"""**Insight:**

1. Recruitment type other have highest promoted employee.
2. Recruitment type sourcing have second promoted employee. 
3. Recruitment type reffered have lowest promoted employee.
"""

data_pct =data.groupby(['rekrutmen','dipromosikan'])['dipromosikan'].count().unstack()
    data_pct.columns = ['Not Promoted', 'Promoted']
    data_pct['Percentage'] = data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])
    data_pct = data_pct.sort_values('Percentage', ascending=True)

    colors = px.colors.sequential.Peach

    fig = go.Figure(data=[go.Pie(labels=data_pct.index, values=data_pct['Percentage'], hole=.5, hoverinfo='label+percent', textinfo='label+percent')])
    fig.update_layout(title_text="percentage promoted employee by recruitment system".title())
    fig.update_traces(marker=dict(colors=colors, line=dict(color='#000000', width=2)))
    fig.show()

"""**Insight:**

1. Recruitment type refered have highest percentage for promoted employee. which means they have good skill and have experience better than other.
2. Recruitment type sourcing and other have no big difference percentage for promoted employee.

##**4.9 Total Training analysis**
"""

data_pct =data.groupby(['jumlah_training','dipromosikan'])['dipromosikan'].count().unstack()
data_pct.columns = ['Not Promoted', 'Promoted']
data_pct['Percentage'] = data_pct['Promoted'] / (data_pct['Promoted'] + data_pct['Not Promoted'])
data_pct = data_pct.sort_values('Percentage', ascending=True)

colors = px.colors.sequential.Peach

fig = go.Figure(data=[go.Pie(labels=data_pct.index, values=data_pct['Promoted'], hole=.5, hoverinfo='label+percent', textinfo='percent')])
fig.update_layout(title_text="Total training and promoted analysis".title())
fig.update_traces(marker=dict(colors=colors, line=dict(color='#000000', width=2)))
fig.show()

"""Employees who have the number of training 1 dominate to be promoted."""

# Compute a simple cross tabulation of two factors
data_pct = pd.crosstab(data['jumlah_training'], data['dipromosikan'])

# Plot the stacked bar plot
plot = data_pct.div(data_pct.sum(1).astype(float), axis = 0).plot(kind = 'bar', 
                                                   stacked = True, 
                                                   color = ['seagreen', 'yellow'],
                                                   figsize = (20, 5))

# Add a title and labels
plt.title('Correlation Total Training and Promotion of employees')
plt.xlabel('Total Training')
plt.legend(['Not Promoted', 'Is Promoted'], loc='lower right')

# Adjust the lables on x-axis
plot.set_xticklabels(
    plot.get_xticklabels(),
    rotation=360,
    horizontalalignment='right');

plt.show()

"""total training has no effect on promotion.

##**4.10 Last Year's Rating Analysis**
"""

data_pct =data.groupby(['rating_tahun_lalu','dipromosikan'])['dipromosikan'].count().unstack()

fig = go.Figure()
fig.add_trace(go.Scatter(x=data_pct.index, y=data_pct[1], mode='lines'))
fig.update_layout(title = "Percentage Promoted Employee by Last Year's Rating", xaxis_tickfont_size = 14, yaxis=dict(title='Total Promoted Employee'))
fig.show()

# Compute a simple cross tabulation of two factors
data_pct = pd.crosstab(data['rating_tahun_lalu'], data['dipromosikan'])

# Plot the stacked bar plot
plot = data_pct.div(data_pct.sum(1).astype(float), axis = 0).plot(kind = 'bar', 
                                                   stacked = True, 
                                                   color = ['seagreen', 'yellow'],
                                                   figsize = (20, 5))

# Add a title and labels
plt.title("Correlation Last year's rating and Promotion of employees")
plt.xlabel("Last Year's Rating")
plt.legend(['Not Promoted', 'Is Promoted'], loc='lower right')

# Adjust the lables on x-axis
plot.set_xticklabels(
    plot.get_xticklabels(),
    rotation=360,
    horizontalalignment='right');

plt.show()

"""##**4.11 Years of Service Analysis**"""

plt.subplots(figsize=(15,5))
sns.distplot(data['masa_kerja'])
plt.title('Distribution of Years of Servie of Employees', fontsize = 30)

"""Most employees work 0-5 years in this company"""

# Compute a simple cross tabulation of two factors
data_pct = pd.crosstab(data['masa_kerja'], data['dipromosikan'])

# Plot the stacked bar plot
plot = data_pct.div(data_pct.sum(1).astype(float), axis = 0).plot(kind = 'bar', 
                                                   stacked = True, 
                                                   color = ['seagreen', 'yellow'],
                                                   figsize = (20, 5))

# Add a title and labels
plt.title("Correlation Years of Service and Promotion of employees")
plt.xlabel("Years of Service")
plt.legend(['Not Promoted', 'Is Promoted'], loc='lower right')

# Adjust the lables on x-axis
plot.set_xticklabels(
    plot.get_xticklabels(),
    rotation=360,
    horizontalalignment='right');

plt.show()

"""Years of service has no effect on promotion.

##**4.12 KPI > 80% Analysis**
"""

fig = px.histogram(data, x=data['KPI_>80%'], color=data['KPI_>80%'], title= 'Total Employee by KPI>80%', color_discrete_sequence= px.colors.qualitative.Pastel2)
  fig.update_layout(showlegend=False)
  fig.show()

"""**Insight:**
* 35,517 employees don't get a KPI value above 80%.
* 19,291 employees get a KPI value above 80%.
* 54% employees dont get KPI value above 80%


"""

temp = pd.DataFrame(data.groupby(['KPI_>80%'], as_index=False)['dipromosikan'].count())

#plotting a pie chart
label=["Not Promoted","Promoted"]
color=['#235711','#82df63']
explode = [0.1, 0.2 ]
plt.figure(figsize=(8,8))
plt.pie(temp['dipromosikan'],labels=label,colors=color,explode=explode,shadow=True,autopct="%.1f%%")
plt.title("Pie Chart of Employees are Promoted by KPI > 80%", fontsize = 15)
plt.axis('off')
plt.legend(title='Promoted Employee')
plt.show()

"""34% of employees with kpi>80% were promoted and the rest were not. It makes a big difference to promoted employees.

##**4.13 Award Analysis**
"""

fig = px.histogram(data, x=data['penghargaan'], color=data['penghargaan'], title= 'Total Employee by Employee Awards',
                     color_discrete_sequence= px.colors.qualitative.Dark24)
  fig.update_layout(showlegend=False)
  fig.show()

"""Very few employees received awards, only 2.37%."""

temp = pd.DataFrame(data.groupby(['penghargaan'], as_index=False)['dipromosikan'].count())

#plotting a pie chart
label=["Not Promoted","Promoted"]
color=['#235711','#82df63']
explode = [0.1, 0.2 ]
plt.figure(figsize=(8,8))
plt.pie(temp['dipromosikan'],labels=label,colors=color,explode=explode,shadow=True,autopct="%.1f%%")
plt.title("Pie Chart of Employees are Promoted by Employee Awards", fontsize = 15)
plt.axis('off')
plt.legend(title='Promoted Employee')
plt.show()

"""From the data above, very few employees are rewarded and promoted. only about 2.3%

##**4.14 Average Training Score Analysis**
"""

plt.subplots(figsize=(15,5))
sns.distplot(data['rata_rata_skor_training'])
plt.title('Distribution of Average Training Score of Employees', fontsize = 30)

# Compute a simple cross tabulation of two factors
data_pct = pd.crosstab(data['rata_rata_skor_training'], data['dipromosikan'])

# Plot the stacked bar plot
plot = data_pct.div(data_pct.sum(1).astype(float), axis = 0).plot(kind = 'bar', 
                                                   stacked = True, 
                                                   color = ['seagreen', 'yellow'],
                                                   figsize = (20, 5))

# Add a title and labels
plt.title("Correlation Average Training Score and Promotion of employees")
plt.xlabel("Average Training Score")
plt.legend(['Not Promoted', 'Is Promoted'], loc='lower right')

# Adjust the lables on x-axis
plot.set_xticklabels(
    plot.get_xticklabels(),
    rotation=360,
    horizontalalignment='right');

plt.show()

"""**Insight:**

1. the difference is less noticeable in employees who receive an average of 39-88 training.
2. It is clear that employees who get an average training score of 89-99 will get a promotion.

The company took the decision to provide promotions, one of which was the value of training.

##**4.15 Total and Perncetage Promoted Employee**
"""

# Count how many employee who are promoted and not promoted
print('Total', len(data[data['dipromosikan'] == 1]), 'employees who are promoted')
print('Total', len(data[data['dipromosikan'] == 0]), 'employees who are not promoted')

# Calculate percentage of promoted employees
print('The percentage of promoted employees is {:.2f}%'.format(100 * data['dipromosikan'].sum() / len(data['dipromosikan'])))

#plotting a pie chart
prmtd = [4668 ,50140]
label=["Promoted","Not Promoted"]
color=['#6389df','#1f2b6c']
explode = [0.1, 0.2 ]
plt.figure(figsize=(8,8))
plt.pie(prmtd,labels=label,colors=color,explode=explode,shadow=True,autopct="%.1f%%")
plt.title("Pie Chart of Employees are Promoted", fontsize = 20)
plt.axis('off')
plt.legend(title='Promoted Employee')
plt.show()

"""8.5% of the total employees who get promotions.

##**4.16 Correlation between Fwatures**
"""

corr=data.corr()
plt.figure(figsize=(12,8))
sns.heatmap(corr,square=True,annot=True, cmap="YlGnBu")

"""The values in each square demonstrate the relationship coefficient. In the event that the relationship coefficient is nearing +1, there's a solid positive connection between the 2 factors. In the event that the relationship coefficient is nearing -1, there's a solid negative connection between the 2 factors. A correlation coefficient close zero implies there's an nonattendance of any relationship between 2 factors.

Most Positive Correlation
* Years of Service and Age Correlation

Most Negative Correlation
* Age and Total Training Correlation

###**4.16.1 Most Positive Correlation**

#### Years of Service and Age Correlation
"""

sns.regplot(y=data['masa_kerja'], x=data['umur'], data=data)

"""Bedasarkan data diatas, karyawan di perusahaan ini loyal.

###**4.16.2 Most Negative Correlation**

#### Age and Total Training Correlation
"""

sns.regplot(x=data['umur'], y=data['jumlah_training'], data=data)

"""semakin tua karyawan maka malas untuk training.

#**5. Feature Engineering**

##**5.1 One Hot Encoding**
"""

data.shape

#using one Hot Encoding for gender column
col_list=['jenis_kelamin']
data_fix = data.copy()

for i in col_list:
  dummy=pd.get_dummies(data[i])
  data_fix=data_fix.join(dummy)
  data_fix.drop(i,axis=1,inplace=True)

data_fix.drop('dipromosikan',axis=1,inplace=True)

"""##**5.2 Label Encoder**"""

lc = LabelEncoder()
data_fix['departemen'] = lc.fit_transform(data['departemen'])
data_fix['wilayah'] = lc.fit_transform(data['wilayah'])
data_fix['pendidikan'] = lc.fit_transform(data['pendidikan'])
data_fix['rekrutmen'] = lc.fit_transform(data['rekrutmen'])

data_fix

"""##**5.3 Data Scaling**"""

minmax = MinMaxScaler()
scaled_data = pd.DataFrame(minmax.fit_transform(data_fix), columns=data_fix.columns)
scaled_data.head()

"""#**6. Feature Selection**"""

x = scaled_data
y = data['dipromosikan']

X_train,X_test,y_train,y_test = train_test_split(x, y, test_size=0.4, random_state = 42)

y_train.value_counts(normalize=True)*100

def oversample(X,y):
    over_sample = SMOTETomek(random_state=42)
    X_over,y_over = over_sample.fit_resample(X,y)
    return X_over,y_over

X_train_os,y_train_os=oversample(X_train ,y_train)

# we can select importance features by using Randomforest Classifier 
forest = GradientBoostingClassifier(loss='exponential',max_features='auto',n_estimators= 500).fit(X_train, y_train)

forest.score(X_test, y_test)

importances = forest.feature_importances_

# create a data frame for visualization.
rank_col = pd.DataFrame({"Features": data_fix.columns, "Importances":importances})
rank_col.set_index('Importances')

# sort in descending order 
rank_col = rank_col.sort_values('Importances',ascending=False)

rank_col = rank_col.reset_index(drop=True)

rank_col

"""I will use top 7 Ranking."""

col_selected = ['rata_rata_skor_training', 'departemen', 'KPI_>80%', 'penghargaan', 'rating_tahun_lalu', 'wilayah', 'masa_kerja']

X_train_os = X_train_os[col_selected]
X_test = X_test[col_selected]
X_train_os.head()

"""#**7. Model Building**

##**7.1 Models Selection**

Selection of the top 7 models based on the value of accuracy.

### **7.1.1 Choose a model based on the value of accuracy**
"""

def classification_model(model, datax, datay, xtest, ytest):
    model.fit(datax, datay.values.ravel())
    pred=model.predict(xtest)
    accuracy=accuracy_score(ytest,pred)
    return accuracy

models=["RandomForestClassifier","Gaussian Naive Bayes","KNN","Logistic_Regression",
        "DecisionTreeClassifier", "XGBoostClassifier", "LGBMClassifier","CatBoostClassifier"]

classification_models = [RandomForestClassifier(n_estimators=100),GB(),knn(n_neighbors=7),LogisticRegression(),
                         DecisionTreeClassifier(), XGBClassifier(), LGBMClassifier(), CatBoostClassifier()]

model_Accuracy = []

for model in classification_models:
    Accuracy= classification_model(model, X_train_os, y_train_os, X_test, y_test)
    model_Accuracy.append(Accuracy)

accuracy_rank = pd.DataFrame(
                { "Model" :models,
                "Accuracy":model_Accuracy
                })
accuracy_rank = accuracy_rank.sort_values('Accuracy', ascending=False).reset_index(drop=True)

accuracy_rank

"""###**7.1.2 Choose a model based on the value of accuracy with Cross Validation**"""

def classification_model_CV(model, datax, datay): 

    scores= cross_val_score(model,datax,datay,scoring="accuracy",cv=10)
    print(scores)
    print('')
    accuracy=scores.mean()
    return accuracy

models=["RandomForestClassifier","Gaussian Naive Bayes","KNN","Logistic_Regression", 
        "DecisionTreeClassifier", "XGBoostClassifier", "LGBMClassifier","CatBoostClassifier"]

classification_models = [RandomForestClassifier(n_estimators=100),GB(),knn(n_neighbors=7),LogisticRegression(),
                         DecisionTreeClassifier(), XGBClassifier(), LGBMClassifier(), CatBoostClassifier()]

model_Accuracy = []

for model,z in zip(classification_models, models):
    print(z) # Print the name of model
    print('')
    Accuracy=classification_model_CV(model,X_train_os,y_train_os)
    
    model_Accuracy.append(Accuracy)

acc_rank_with_CV = pd.DataFrame(
    { "Model" :models,
     "Accuracy":model_Accuracy
     
    })
acc_rank_with_CV.sort_values(by="Accuracy",ascending=False).reset_index(drop=True)

rank =accuracy_rank.merge(acc_rank_with_CV, how='inner', on='Model')
rank.columns = ['Model', 'Acc', 'Acc with CV']
rank

"""We will use the top 5 models based on accuracy values.

  1. CatBoost Classifier
  2. LGBM Classifier
  3. Random Forest Classifier
  4. DecisionTreeClassifier
  5. XGBoost Classifier

##**7.2 Hyper-Parameter Tuning using HalvingGridSearchCV**
"""

def Classification_model_HalvingGridSearchCV(model, datax, datay, params):
    
    clf = HalvingGridSearchCV(model, params, scoring = 'accuracy', n_jobs=-1, min_resources="exhaust", factor=3)
    clf.fit(datax, datay.values.ravel())


    print("best score is :")
    print(clf.best_score_)
    print('')
    print("best params is :")
    print(clf.best_params_)
    print("best estimator is :")
    print(clf.best_estimator_)
    

    return (clf.best_score_)

models=["CatBoostClassifier", "LGBMClassifier", "RandomForestClassifier","DecisionTreeClassifier", "XGBoostClassifier"]

model_score=[]

model= CatBoostClassifier()

param_grid = {'n_estimators'  : [400, 600],
              'learning_rate' : [0.01,0.1],
              'random_state' : [42]
              }

score=Classification_model_HalvingGridSearchCV(model,X_train_os,y_train_os, param_grid)
model_score.append(score)

model = LGBMClassifier()

param_grid = {
              'learning_rate': [0.05, 0.1], 
              'n_estimators': [100, 300, 500], 
              'num_leaves': [300],
              'boosting_type' : ['dart'],
              'random_state' : [42]
              }
score=Classification_model_HalvingGridSearchCV(model,X_train_os, y_train_os, param_grid)
model_score.append(score)

model = RandomForestClassifier()
param_grid = {
              'n_estimators': [800, 1100], 
              'min_samples_split': [5],
              'min_samples_leaf': [1],
              'max_depth': [15, 17, 20],
              'random_state' : [42]}
score=Classification_model_HalvingGridSearchCV(model,X_train_os, y_train_os, param_grid)
model_score.append(score)

model=DecisionTreeClassifier()

max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]

param_grid={
            'max_depth': max_depth,
            'min_samples_split': [5, 10, 15],
            'min_samples_leaf': [2, 5],
            'random_state' : [42]}
score=Classification_model_HalvingGridSearchCV(model,X_train_os, y_train_os, param_grid)
model_score.append(score)

model=XGBClassifier()
param_grid={
            'max_depth': [8, 10, 13],
            'subsample': [0.7],
            'min_child_weight': [1],
            'colsample_bytree': [0.5], 
            'n_estimators': [100, 500],
            'learning_rate': [0.05, 0.1],
            'random_state' : [42]}
score=Classification_model_HalvingGridSearchCV(model,X_train_os, y_train_os, param_grid)
model_score.append(score)

hyper_tuning = pd.DataFrame(
    { "Model" :models,
     "Accuracy":model_score
     
    })
hyper_tuning.sort_values(by="Accuracy",ascending=False).reset_index(drop=True)

"""From this hyper-parameter tuning, the best Accuracy value is LGBMClassifier.

##**7.3 AUC and ROC Analysis**
"""

# model initialization 
model1 = LGBMClassifier(boosting_type='dart', learning_rate=0.05, n_estimators=500, num_leaves=300, random_state=42)
model2 = CatBoostClassifier(learning_rate= 0.1, n_estimators= 600, random_state= 42)
model3 = XGBClassifier(colsample_bytree=0.5, max_depth=13, random_state=42, subsample=0.7)
model4 = RandomForestClassifier(max_depth=20, min_samples_split=5, n_estimators=800, random_state=42)
model5 = DecisionTreeClassifier(max_depth=30, min_samples_leaf=2, min_samples_split=5, random_state=42)


# model fit
model1.fit(X_train_os, y_train_os.values.ravel())
model2.fit(X_train_os, y_train_os.values.ravel())
model3.fit(X_train_os, y_train_os.values.ravel())
model4.fit(X_train_os, y_train_os.values.ravel())
model5.fit(X_train_os, y_train_os.values.ravel())


# predict probabilities
pred_prob1 = model1.predict_proba(X_test)
pred_prob2 = model2.predict_proba(X_test)
pred_prob3 = model3.predict_proba(X_test)
pred_prob4 = model4.predict_proba(X_test)
pred_prob5 = model5.predict_proba(X_test)

fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)
fpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label=1)
fpr4, tpr4, thresh4 = roc_curve(y_test, pred_prob4[:,1], pos_label=1)
fpr5, tpr5, thresh5 = roc_curve(y_test, pred_prob5[:,1], pos_label=1)

# roc curve for tpr = fpr 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)

# auc scores
auc_score1 = roc_auc_score(y_test, pred_prob1[:,1])
auc_score2 = roc_auc_score(y_test, pred_prob2[:,1])
auc_score3 = roc_auc_score(y_test, pred_prob3[:,1])
auc_score4 = roc_auc_score(y_test, pred_prob4[:,1])
auc_score5 = roc_auc_score(y_test, pred_prob5[:,1])

print(auc_score1, auc_score2, auc_score3, auc_score4, auc_score5)

plt.style.use('seaborn')

# plot roc curves
plt.figure(figsize=(20,8))
plt.plot(fpr1, tpr1, linestyle='--',color='salmon', label='LGBM Classifier')
plt.plot(fpr2, tpr2, linestyle='--',color='olive', label='CatBoost Classifier')
plt.plot(fpr3, tpr3, linestyle='--',color='lawngreen', label='XGB Classifier')
plt.plot(fpr4, tpr4, linestyle='--',color='red', label='Random Forest Classifier')
plt.plot(fpr5, tpr5, linestyle='--',color='magenta', label='Decision Tree Classifier')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

"""The graph above is a graph of ROC (Receiver Operating Characteristics). Where the y-axis represents the True Positive Rate(TPR), the x-axis represents the False Positive Rate(FPR). True Positive Rate is obtained from the Sensitivity value, while the False Positive Rate is obtained from 1-Specificity.

AUC (Area Under the Curve) is the area under the ROC curve. The greater the AUC value or the area under the ROC curve, the better the model.


From the picture above, the model closest to the value 1 (top left corner) is the model from the **CatBoost Classifier**. Because the AUC value of the **CatBoost Classifier** is the largest compared to the others.
"""