# -*- coding: utf-8 -*-
"""Copy of tes submisiion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g88EcOmIm7yOPieoVK3t1flVpeiDahhr

#**NLP using TensorFlow**
Dataset: https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews
"""

#Import dataset

import pandas as pd
df = pd.read_csv("/content/spam_ham_dataset.csv")

df.head()

df = df.drop(['Unnamed: 0', 'label_num'], axis=1)
df = df.rename(columns = {"text" : "Message"})
df = df.rename(columns = {"label" : "Label"})
df.info()

ctg = pd.get_dummies(df['Label'])
df = pd.concat([df, ctg], axis=1)
df = df.drop(columns='Label')

df.head()

#panjang data
len(df)

df.isna().sum()

X = df['Message'].values
y = df[['ham','spam']].values

print(X.shape, y.shape)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_words = 10000
max_len = 200

tokenizer = Tokenizer(num_words=max_words, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenizer.fit_on_texts(X) 
 
X = tokenizer.texts_to_sequences(X)
 
X = pad_sequences(X, maxlen=max_len)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

import tensorflow as tf
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence

def model():
    inputs = Input(shape=[max_len])
    layer = Embedding(max_words, 50, input_length=max_len)(inputs)
    layer = LSTM(256, dropout=0.2, recurrent_dropout=0.2)(layer)
    layer = Dense(256)(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.5)(layer)
    layer = Dense(2)(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=layer)
    return model

model = model()
model.compile(loss='categorical_crossentropy',optimizer= RMSprop(),metrics=['accuracy'])
print(model.summary())

# callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')==1.0):
      self.model.stop_training = True
      print("\nAkurasi mencapai 100%!")
callbacks = myCallback()

num_epochs = 200
history = model.fit(X_train, y_train, batch_size=32,
                    epochs=num_epochs, verbose=2,
                    validation_data=(X_test, y_test), callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'])
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.show()